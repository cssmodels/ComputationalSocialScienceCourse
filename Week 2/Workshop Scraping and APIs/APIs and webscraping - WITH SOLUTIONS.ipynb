{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75be9f4-4d59-4f01-899a-1604fdbf3d03",
   "metadata": {},
   "source": [
    "# Web Scraping and APIs - Requests, Selenium and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f05bb5-09df-43ea-b68a-605de67c7098",
   "metadata": {},
   "source": [
    "This notebook provides a quick introduction to web scraping with Python, as part of the University of Amsterdam course Computational Social Science Analysis.\n",
    "\n",
    "We will start by a simple introduction to using APIs and web scraping. We will learn scraping using requests and selenium. We will then build up toward a small research project that uses scraping and API.\n",
    "\n",
    "It has been developed by Petter T√∂rnberg. p.tornberg@uva.nl\n",
    "Version 1.0. 2024-04-04 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with APIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An API, or Application Programming Interface, allows different software applications to talk to each other, sharing data and functionalities easily. Developers use APIs to access features or data from other services, enabling more complex and feature-rich applications. Essentially, APIs serve as bridges between different software, making it possible for them to interact and share resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start with getting data from a simple API. It's easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using a simple API:  How's the weather?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch data from any API or website, we can use the requests package. The requests package abstracts the complexities of making requests behind simple API methods, allowing developers to send HTTP/1.1 requests with various methods like GET, POST, PUT, and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will use OpenWeatherMap.\n",
    "\n",
    "#### API Documentation: _Read The Fine Manual! (RTFM)_\n",
    "Public APIs always come with documentation that describes how to use the API, and what data you can expect. \n",
    "\n",
    "To find the OpenWeatherMap API, you can go to:\n",
    "https://openweathermap.org/api\n",
    "\n",
    "\n",
    "#### Getting the current weather\n",
    "We will here use the current weather function, to get the current weather in Amsterdam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the result from the API:\n",
      "{\"coord\":{\"lon\":4.8897,\"lat\":52.374},\"weather\":[{\"id\":802,\"main\":\"Clouds\",\"description\":\"scattered clouds\",\"icon\":\"03n\"}],\"base\":\"stations\",\"main\":{\"temp\":285.35,\"feels_like\":284.74,\"temp_min\":284.18,\"temp_max\":285.97,\"pressure\":1005,\"humidity\":81},\"visibility\":10000,\"wind\":{\"speed\":7.2,\"deg\":240},\"clouds\":{\"all\":40},\"dt\":1712256371,\"sys\":{\"type\":2,\"id\":2012552,\"country\":\"NL\",\"sunrise\":1712207222,\"sunset\":1712254754},\"timezone\":7200,\"id\":2759794,\"name\":\"Amsterdam\",\"cod\":200}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"de26752686c975de6a1c38a998f50fec\"\n",
    "city_name = \"Amsterdam\"\n",
    "base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "\n",
    "# Complete URL for the API call\n",
    "url = f\"{base_url}q={city_name}&appid={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Here is the result from the API:\")\n",
    "    print(response.text)\n",
    "    json_string = response.text\n",
    "else:\n",
    "    print(\"Error: Unable to get data from OpenWeatherMap API! :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huh, what is this strange text?\n",
    "As you can see, the result we get is in a particular text format. This format is called JSON (pronounced \"Jason\"), which is used by most APIs - both internal and public.\n",
    "\n",
    "JSON (JavaScript Object Notation) is a data interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is primarily used to transmit data between a server and a web application, serving as an alternative to XML, and is widely used for representing structured data and exchanging information in web development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, JSON is very easy to parse using Python. We may for instance turn it into a dict. We use the json library to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Amsterdam-----------\n",
      "Temperature: 285.35K\n",
      "Humidity: 81%\n",
      "Weather: Clouds\n",
      "Description: scattered clouds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(json_string)\n",
    "\n",
    "# Now parsed_data is a Python dictionary containing the data from the JSON string\n",
    "main = data['main']\n",
    "weather = data['weather']\n",
    "print(f\"{city_name:-^30}\")\n",
    "print(f\"Temperature: {main['temp']}K\")\n",
    "print(f\"Humidity: {main['humidity']}%\")\n",
    "print(f\"Weather: {weather[0]['main']}\")\n",
    "print(f\"Description: {weather[0]['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Your turn! Get the forecast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your task is to get the \"5 day / 3 hour forecast data\" from the API, to figure out how the weather in Amsterdam will be in the coming days. Read the manual!\n",
    "\n",
    "The goal is to print the date in the following format: \n",
    "- On 2023-10-06 12:00:00 the temperature will be 15 C\n",
    "- On 2023-10-06 15:00:00 the temperature will be  4 C\n",
    "\n",
    "etc.\n",
    "\n",
    "There are two extra challenges here. \n",
    "First, the datetime is a timestamp (a float value representing the number of seconds since January 1, 1970, the Unix epoch), which you will need to convert to a readable date.\n",
    "\n",
    "Second, you will need to convert the temperature from Kelvin to Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some help: a function to convert timestamp to date-time string\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_timestamp(dt):\n",
    "    dt_object = datetime.utcfromtimestamp(dt)\n",
    "    formatted_date = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 09:00:00: 13C\n",
      "2024-04-05 12:00:00: 13C\n",
      "2024-04-05 15:00:00: 14C\n",
      "2024-04-05 18:00:00: 13C\n",
      "2024-04-05 21:00:00: 13C\n",
      "2024-04-06 00:00:00: 13C\n",
      "2024-04-06 03:00:00: 12C\n",
      "2024-04-06 06:00:00: 12C\n",
      "2024-04-06 09:00:00: 17C\n",
      "2024-04-06 12:00:00: 22C\n",
      "2024-04-06 15:00:00: 22C\n",
      "2024-04-06 18:00:00: 18C\n",
      "2024-04-06 21:00:00: 15C\n",
      "2024-04-07 00:00:00: 15C\n",
      "2024-04-07 03:00:00: 12C\n",
      "2024-04-07 06:00:00: 12C\n",
      "2024-04-07 09:00:00: 14C\n",
      "2024-04-07 12:00:00: 17C\n",
      "2024-04-07 15:00:00: 17C\n",
      "2024-04-07 18:00:00: 14C\n",
      "2024-04-07 21:00:00: 12C\n",
      "2024-04-08 00:00:00: 11C\n",
      "2024-04-08 03:00:00: 10C\n",
      "2024-04-08 06:00:00: 10C\n",
      "2024-04-08 09:00:00: 12C\n",
      "2024-04-08 12:00:00: 11C\n",
      "2024-04-08 15:00:00: 13C\n",
      "2024-04-08 18:00:00: 13C\n",
      "2024-04-08 21:00:00: 12C\n",
      "2024-04-09 00:00:00: 9C\n",
      "2024-04-09 03:00:00: 8C\n",
      "2024-04-09 06:00:00: 8C\n",
      "2024-04-09 09:00:00: 9C\n",
      "2024-04-09 12:00:00: 11C\n",
      "2024-04-09 15:00:00: 11C\n",
      "2024-04-09 18:00:00: 8C\n",
      "2024-04-09 21:00:00: 7C\n",
      "2024-04-10 00:00:00: 7C\n",
      "2024-04-10 03:00:00: 7C\n",
      "2024-04-10 06:00:00: 7C\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "import requests\n",
    "\n",
    "api_key = \"de26752686c975de6a1c38a998f50fec\"\n",
    "city_name = \"Amsterdam\"\n",
    "base_url = \"http://api.openweathermap.org/data/2.5/forecast?\"\n",
    "\n",
    "# Complete URL for the API call\n",
    "url = f\"{base_url}q={city_name}&appid={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    j = response.json()\n",
    "    for l in j['list']:\n",
    "        print(f\"{parse_timestamp(l['dt'])}: {int(l['main']['temp']-273)}C\")\n",
    "else:\n",
    "    print(\"Error: Unable to get data from OpenWeatherMap API! :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a sense of how to get data from a simple API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using _requests_ on a normal website instead. It's quite similar! We here use it to fetch the CSS programme website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ce4731-4b1c-4d47-8778-c07ad1b5b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the result:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<!doctype html>\n",
      "<html class=\"no-js\" lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"utf-8\"/>\n",
      "\n",
      "    <title>Bachelor's Computational Social Science - University of Amsterdam</title>\n",
      "            <link rel=\"canonical\" href=\"https://www.uva.nl/en/programmes/bachelors/computational-social-science/computational-so ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.uva.nl/en/programmes/bachelors/computational-social-science/computational-social-science.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Here is the result:\")\n",
    "    print(f\"{response.text[:300]} ...\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcc01b-e205-43bc-80be-76c25ef481fa",
   "metadata": {},
   "source": [
    "As you can see, the result is in HTML: the simple markup language that the internet is built on.\n",
    "\n",
    "To get data from HTML, we therefore need to parse the HTML to fetch the data that we are interested in. This is core to all scraping.\n",
    "\n",
    "We therefore need a way of parsing the HTML to get the data that we are interested in.\n",
    "\n",
    "This is where BeautifulSoup comes in!\n",
    "\n",
    "### Requests + BeautifulSoup: Parsing HTML and XML\n",
    "\n",
    "Beautiful Soup is a Python library designed for web scraping, to extract data from HTML and XML files. It provides ways for navigating, searching, and modifying \"parse trees\", making it easy to extract necessary information from a web page. \n",
    "\n",
    "Here follows a list of some of the main functionalities and methods provided by Beautiful Soup:\n",
    "\n",
    "1. Parsing\n",
    "- BeautifulSoup(markup, parser): This is the constructor function that takes in the HTML or XML content as a string and a string representing the parser to be used (like 'html.parser', 'lxml', 'lxml-xml', or 'html5lib').\n",
    "\n",
    "2. Navigating the Parse Tree\n",
    "- .contents: Provides a list of a tag‚Äôs children.\n",
    "- .children: Similar to .contents but returns an iterator over the tag‚Äôs children.\n",
    "- .descendants: Iterates over all of a tag‚Äôs children, recursively.\n",
    "- .parent and .parents: Navigate up the parse tree to find parent tags.\n",
    "\n",
    "3. Searching the Tree\n",
    "- .find_all(name, attrs, recursive, string, limit, **kwargs): Searches the tree for tags matching the specified filters, and returns a ResultSet of matching tags.\n",
    "- .find(name, attrs, recursive, string, **kwargs): Returns the first tag found that matches the specified filters.\n",
    "- .select(css_selector): Searches the tree for tags matching the given CSS selector(s).\n",
    "\n",
    "4. Accessing Tag Attributes\n",
    "- .name: Accesses the name of the tag.\n",
    "- .attrs: Accesses a dictionary of the tag‚Äôs attributes.\n",
    "\n",
    "5. Manipulating the Tree\n",
    "- .append(): Appends a tag or string to a tag‚Äôs contents.\n",
    "- .insert(): Inserts a tag or string within a tag‚Äôs contents at a specified position.\n",
    "- .extract(): Extracts a tag from the tree, modifying the tree.\n",
    "- .decompose(): Removes a tag from the tree and then completely destroys it along with its contents.\n",
    "\n",
    "6. Get Text\n",
    "- .get_text(separator, strip, types): Extracts all text from a document or tag, optionally separated by a separator string and optionally stripping whitespace.\n",
    "\n",
    "7. Formatting Output\n",
    "- .prettify(): Returns a string representing the tag‚Äôs contents, formatted as nicely indented HTML.\n",
    "\n",
    "8. Encoding\n",
    "- .encode(encoding, formatter): Returns a string or bytes representing the tag, encoded in the given encoding.\n",
    "\n",
    "9. Decoding\n",
    "- .decode(formatter): Returns a Unicode string representing the tag.\n",
    "- Beautiful Soup provides flexible and user-friendly ways to navigate and search the parse tree, making it a powerful tool for web scraping needs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddce38-4b45-496d-a628-8679d72bb14d",
   "metadata": {},
   "source": [
    "Let's use it to fetch the description of the CSS program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3cee4-234c-497c-bb38-538ce7b49bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if you do not already have it\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5b8c36-2633-4a11-8749-4b3bcea4b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the library\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2c1c4-6257-4dc1-b227-65fd787392eb",
   "metadata": {},
   "source": [
    "### Parsing a simple example website with beautifulsoup\n",
    "As you may know, HTML is hierarchically structured  - sometimes referred to as an HTML parse tree or the DOM tree. The DOM is a tree data structure that represents the hierarchical structure of an HTML document. Each node in the tree corresponds to an element (or \"tag\") in the HTML document, and the edges represent the nesting relationships between the elements. The root of the tree is typically the <html> tag, and it has child nodes representing the head and body of the HTML document, and those child nodes, in turn, have their own child nodes representing nested elements within them.\n",
    "\n",
    "For example, consider a simple HTML document:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e8e9ca-7ad7-4d4e-8553-d42a9ec17a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplehtml = '''<html>\n",
    "    <head>\n",
    "        <title>My Page</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Welcome to My Page</h1>\n",
    "        <p>This is a paragraph.</p>\n",
    "    </body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea932f-7c16-47a4-b702-8c07f63c28cf",
   "metadata": {},
   "source": [
    "The HTML parse tree for this document would have the <html> tag as the root, with child nodes \\<head> and \\<body>. The \\<head> node would have a child node \\<title>, and the \\<body> node would have child nodes \\<h1> and \\<p>. Each node may also contain text nodes representing the text content within the tags.\n",
    "\n",
    "Creating a parse tree from an HTML document is part of web scraping to navigate and extract information from the document structure programmatically. This is where BeautifulSoup comes in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81cc74e2-cdb5-4987-a1ca-23d63a196374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Tag: <head>\n",
      "<title>My Page</title>\n",
      "</head>\n",
      "Title Tag: <title>My Page</title>\n",
      "Title Text: My Page\n",
      "Body Tag: <body>\n",
      "<h1>Welcome to My Page</h1>\n",
      "<p>This is a paragraph.</p>\n",
      "</body>\n",
      "H1 Tag: <h1>Welcome to My Page</h1>\n",
      "H1 Text: Welcome to My Page\n",
      "Paragraph Tag: <p>This is a paragraph.</p>\n",
      "Paragraph Text: This is a paragraph.\n",
      "Parent of h1: body\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(simplehtml, 'html.parser')\n",
    "\n",
    "# Navigating to the <head> tag\n",
    "head = soup.head\n",
    "print(\"Head Tag:\", head)\n",
    "\n",
    "# Navigating to the <title> tag inside <head>\n",
    "title = head.title\n",
    "print(\"Title Tag:\", title)\n",
    "\n",
    "# Extracting text from the <title> tag\n",
    "title_text = title.get_text()\n",
    "print(\"Title Text:\", title_text)\n",
    "\n",
    "# Navigating to the <body> tag\n",
    "body = soup.body\n",
    "print(\"Body Tag:\", body)\n",
    "\n",
    "# Navigating to the <h1> tag inside <body>\n",
    "h1 = body.h1\n",
    "print(\"H1 Tag:\", h1)\n",
    "\n",
    "# Extracting text from the <h1> tag\n",
    "h1_text = h1.get_text()\n",
    "print(\"H1 Text:\", h1_text)\n",
    "\n",
    "# Navigating to the <p> tag inside <body>\n",
    "paragraph = body.p\n",
    "print(\"Paragraph Tag:\", paragraph)\n",
    "\n",
    "# Extracting text from the <p> tag\n",
    "paragraph_text = paragraph.get_text()\n",
    "print(\"Paragraph Text:\", paragraph_text)\n",
    "\n",
    "# We can navigate upward in tree with parent\n",
    "print(\"Parent of h1:\", h1.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parse a simple website\n",
    "Your task is to parse the following simple website using beautifulsoup, and extract a dataframe that has the products listed, with their name, description, and price in separate columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how the website looks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <title>Simple Website Example</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>Welcome to Our Simple Website</h1>\n",
       "<p>This is a demonstration of a simple HTML website designed for parsing practice.</p>\n",
       "<h2>About Us</h2>\n",
       "<p>We are a team dedicated to learning web scraping with BeautifulSoup.</p>\n",
       "<h3>Contact Information</h3>\n",
       "<p>Email us at: <a href=\"mailto:info@example.com\">info@example.com</a></p>\n",
       "<h2>Our Products</h2>\n",
       "<table border=\"1\">\n",
       "    <tr>\n",
       "        <th>Product Name</th>\n",
       "        <th>Description</th>\n",
       "        <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Product 1</td>\n",
       "        <td>An essential item for beginners.</td>\n",
       "        <td>$19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Product 2</td>\n",
       "        <td>A must-have for advanced users.</td>\n",
       "        <td>$29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Product 3</td>\n",
       "        <td>Now with bacon-flavor!</td>\n",
       "        <td>$39.99</td>\n",
       "    </tr>\n",
       "</table>\n",
       "\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "website_html = '''<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Simple Website Example</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Welcome to Our Simple Website</h1>\n",
    "<p>This is a demonstration of a simple HTML website designed for parsing practice.</p>\n",
    "<h2>About Us</h2>\n",
    "<p>We are a team dedicated to learning web scraping with BeautifulSoup.</p>\n",
    "<h3>Contact Information</h3>\n",
    "<p>Email us at: <a href=\"mailto:info@example.com\">info@example.com</a></p>\n",
    "<h2>Our Products</h2>\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Product Name</th>\n",
    "        <th>Description</th>\n",
    "        <th>Price</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Product 1</td>\n",
    "        <td>An essential item for beginners.</td>\n",
    "        <td>$19.99</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Product 2</td>\n",
    "        <td>A must-have for advanced users.</td>\n",
    "        <td>$29.99</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Product 3</td>\n",
    "        <td>Now with bacon-flavor!</td>\n",
    "        <td>$39.99</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>'''\n",
    "print(\"This is how the website looks:\")\n",
    "display(HTML(website_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product 1</td>\n",
       "      <td>An essential item for beginners.</td>\n",
       "      <td>$19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product 2</td>\n",
       "      <td>A must-have for advanced users.</td>\n",
       "      <td>$29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product 3</td>\n",
       "      <td>Now with bacon-flavor!</td>\n",
       "      <td>$39.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Product Name                       Description   Price\n",
       "0    Product 1  An essential item for beginners.  $19.99\n",
       "1    Product 2   A must-have for advanced users.  $29.99\n",
       "2    Product 3            Now with bacon-flavor!  $39.99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd \n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(website_html, 'html.parser')\n",
    "\n",
    "# [YOUR SOLUTION HERE]\n",
    "#----- SOLUTION -----\n",
    "# Find the table containing products\n",
    "product_table = soup.find('table')\n",
    "\n",
    "# Extract the rows in the table, skipping the header row\n",
    "rows = product_table.find_all('tr')[1:]\n",
    "\n",
    "# Extract the data for each row\n",
    "products = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    product_name = cols[0].text\n",
    "    description = cols[1].text\n",
    "    price = cols[2].text\n",
    "    products.append([product_name, description, price])\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(products, columns=['Product Name', 'Description', 'Price'])\n",
    "#------ /SOLUTION -----\n",
    "\n",
    "# Dataframe that has the products\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced0d0c-4103-40ea-9863-9c88d630255b",
   "metadata": {},
   "source": [
    "## A realistic example: Parsing the CSSci website\n",
    "Let's use this to parse the CSSci website we fetched earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6779b7fe-bb07-4d4b-b05c-a284e554190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the html\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b1d1a-7abe-4fc4-aafd-a1f81297cec7",
   "metadata": {},
   "source": [
    "\n",
    "So, how do we find the element we want, in a complex HTML website like UvA.nl?\n",
    "\n",
    "One way is to find out the CSS selector for the element you are seeking, you can use the extremely useful Chrome Developer Tools. Open Chrome. Go to the website. Go to Menu > More Tools > Developer Tools.\n",
    "\n",
    "You can use the \"Select element\", represented by a diagonal arrow in the upper right corner. \n",
    "\n",
    "Click the element on the page that you are interested in: the main description.\n",
    "\n",
    "You can now see that the description text is inside a _p_ of class _lead_ which is inside a _div_ with class 'c-programmepageheader'.\n",
    "\n",
    "We can use the CSS selector to select this element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feb3a688-b4f9-42b8-a51a-58b10e2cb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = soup.select('div.c-programmepageheader p.lead')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea61979-c815-4970-8bef-e0a7c314360b",
   "metadata": {},
   "source": [
    "The result is a list with a single element in it: the element we are looking for. To get the text of this element, we simply need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5263cc46-0c74-4730-b5b5-69c4aa0c7afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to learn how to make the world a better place by using digital technology? Are you eager to learn all about data science? Do you have a hands on attitude and excellent team spirit? Digitisation poses new questions and challenges for our societies, but it also brings new opportunities to intervene in societal issues such as social inequality and climate change. Scientifically grounded in social sciences, humanities and information science, as a student of Computational Social Science you will develop critical skills to analyse digital phenomena using advanced data science techniques combined with rigorous research methods. You will also learn how to design digital interventions and effect change to benefit society.\n"
     ]
    }
   ],
   "source": [
    "print(description[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67329742-4dae-4e5d-b1f8-ddfdf709d85b",
   "metadata": {},
   "source": [
    "### Exercise: \"Is Computational Social Science right for you?\"\n",
    "Your task is to find the list of four points on the UvA CSSci website that answers the questions of whether the CSS programme is right for you. \n",
    "\n",
    "Use what you've learned to fetch this list, and print each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d8f309-9ade-4223-a0b6-8ae876c60876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "# --- SOLUTION ---\n",
    "points = soup.select('div.c-richtext__rowcontainer div div ul li')\n",
    "print(f\"Here are the {len(points)} points to decide if CSS is right for you:\")\n",
    "for i,point in enumerate(points):\n",
    "    print(f\"{i}. {point.text}\")\n",
    "# --- /SOLUTION ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1464ef5-39f2-4aa9-b47b-24a72b64fb8c",
   "metadata": {},
   "source": [
    "## Selenium: Scraping dynamic pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43011b-1716-4e23-9f5a-fd6106a597aa",
   "metadata": {},
   "source": [
    "While _requests_ is a powerful tool for getting static HTML pages, most websites these days are not static HTML. _requests_ cannot handle Javascript pages or dynamic content.\n",
    "\n",
    "This is where Selenium comes in. Selenium automates a web browser, allowing it to interact with the JavaScript and dynamically loaded content on the webpage, thereby providing access to content modified or loaded by JavaScript after the initial page load. Selenium can also automate interactions with the website, such as clicking buttons, filling out forms, or navigating through pages. For these points, the requests library alone would be insufficient, as it cannot interact with webpage elements or execute user-like actions.\n",
    "\n",
    "Selenium in other words runs a complete web browser, and automates clicking on the websites. This allows it to scrape nearly any website. But it also means that it is relatively heavy and slow, compared to a _requests_ based solution. \n",
    "\n",
    "*Takeaway: use requests when dealing with static websites or APIs. Use Selenium when dealing with more complex dynamic websites.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462a0a5-6785-43bc-8d8f-976932bb7e77",
   "metadata": {},
   "source": [
    "### Installing Selenium\n",
    "\n",
    "Installing selenium can be a bit of a challenge on its own, as it is dependent on having a chrome/chromium browser installed. Expect a bit of fiddling!\n",
    "\n",
    "Regardless of the OS, you first need to install the Selenium python package: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "\n",
    "!pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5cc8a21-8e22-4fc1-8692-729710ad363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0a3d9ce-8716-4ca1-baf4-29b080def34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You should now see a web browser opening on your computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00fbe569-32af-48fa-a64f-250e938e8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first close the cookie window by click the \"no thanks\" button\n",
    "button = driver.find_element('id','W0wltc')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd3aca4-0c7d-404e-97ea-1e04923dcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the search bar using the name of the input field\n",
    "search_bar = driver.find_element(\"name\", \"q\")\n",
    "\n",
    "# Type the search term and hit ENTER\n",
    "search_bar.send_keys(\"university of amsterdam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca49e838-3ee2-46ab-ae31-62853cc24ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click enter!\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for some time to let the results load\n",
    "time.sleep(2) \n",
    "\n",
    "#The page will now have made the search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f2d2183-87aa-446b-a286-d458a05aa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the titles and URLs of the search hits.\n",
    "results = driver.find_elements(by=By.CSS_SELECTOR,value='a h3') #Select all links under the div with id search: these are the search results.\n",
    "\n",
    "# Extract and print the top 10 hits \n",
    "for result in results:\n",
    "    if len(result.text)>0:\n",
    "        #This is how to get the parent element in selenium. We want the <a> to get the URL.\n",
    "        parent_element = result.find_element(by=By.XPATH, value='..')\n",
    "        \n",
    "        print(f\"{result.text}. {parent_element.get_attribute('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c39cfa-e646-406b-b8b8-3b602d587997",
   "metadata": {},
   "source": [
    "-----\n",
    "To get more results, we can scroll to the bottom of the page, and wait for a moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7ad640-6501-4638-9e1d-9156ff4a3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Wait for a while to allow contents to load, if any\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a1f881e-3a10-48ba-b785-1b7a2db08fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the same code as above, we now get a larger number of results!\n",
    "results = driver.find_elements(by=By.CSS_SELECTOR,value='a h3') #Select all links under the div with id search: these are the search results.\n",
    "\n",
    "# Extract and print the top 10 hits \n",
    "i=0\n",
    "for result in results:\n",
    "    if len(result.text)>0:\n",
    "        i+=1\n",
    "        #This is how to get the parent element in selenium. We want the <a> to get the URL.\n",
    "        parent_element = result.find_element(by=By.XPATH, value='..')\n",
    "        \n",
    "        print(f\"{i}. {result.text}. {parent_element.get_attribute('href')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56302bb3-8d32-4269-bd1d-51f50297ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e38133-6c11-4400-b3bd-4db1d3e66da8",
   "metadata": {},
   "source": [
    "### Exercise: Find the Google ranking of UvA's Computational Social Science. \n",
    "\n",
    "Your task is to adapt the code to use selenium to search for 'computational social science', and to find where UvA shows up in the search ranking. \n",
    "\n",
    "1. Use Selenium to open google.com. Close the popup, and search for 'computational social science'\n",
    "\n",
    "2. Your script should keep scrolling in the search result until it finds a search result with an HREF that includes with 'uva.nl'. \n",
    "\n",
    "3. It should then print the number in the list of the identified link, and how many pages you had to scroll. For instance, if it is the first link found, your code should output: 'UvA was number 1 link for search result, on page 1 in the Google ranking!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec6d3682-27a8-4103-bc67-9659405615ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# This function fetches\n",
    "# Takes: a search term string\n",
    "# Returns: the link in order it was found on, and the page it was found on. e.g., \"return rank, page\"\n",
    "# If it does not find the link in the first 5 pages, \"return None, None\"\n",
    "def find_google_ranking(search_term,url_to_look_for):\n",
    "    #[YOUR CODE HERE]\n",
    "\n",
    "rank, page = find_google_ranking(\"computational social science\",\"uva.nl\")\n",
    "\n",
    "if rank is None:\n",
    "    print(f\"Uva.nl was not listed in the first {how_many_pages_to_try} pages! :( We need to work on our SEO!\")\n",
    "else:\n",
    "    print(f\"UvA was number {rank} link for search result, on page {page} in the Google ranking!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# This function fetches\n",
    "# Takes: a search term string\n",
    "# Returns: the link in order it was found on, and the page it was found on.\n",
    "def find_google_ranking(search_term,url_to_look_for):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(\"https://www.google.com\")\n",
    "\n",
    "    time.sleep(5) \n",
    "\n",
    "    #Let's first close the cookie window by click the \"no thanks\" button\n",
    "    button = driver.find_element('id','W0wltc')\n",
    "    button.click()\n",
    "\n",
    "    time.sleep(3) \n",
    "\n",
    "    # Find the search bar using the name of the input field\n",
    "    search_bar = driver.find_element(\"name\", \"q\")\n",
    "\n",
    "    # Type the search term and hit ENTER\n",
    "    search_bar.send_keys(search_term)\n",
    "\n",
    "    #Click enter!\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for some time to let the results load\n",
    "    time.sleep(2) \n",
    "\n",
    "    how_many_pages_to_try=5\n",
    "    #The page will now have made the search\n",
    "    # We go over pages by scrolling down\n",
    "    for page in range(how_many_pages_to_try):\n",
    "\n",
    "        # Get search results\n",
    "        results = driver.find_elements(by=By.CSS_SELECTOR,value='a h3') \n",
    "\n",
    "        #Extract hits\n",
    "        for result_nr,result in enumerate(results):\n",
    "            if len(result.text)>0:\n",
    "\n",
    "                parent_element = result.find_element(by=By.XPATH, value='..')\n",
    "\n",
    "                #Does it contain the URL to uva?\n",
    "                if parent_element.get_attribute('href') is not None and url_to_look_for in parent_element.get_attribute('href'):\n",
    "                    #We found it!\n",
    "                    driver.quit()\n",
    "                    return result_nr+1, page+1\n",
    "\n",
    "        #Scroll to next page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2) # Wait for a while to allow contents to load\n",
    "\n",
    "    driver.quit()\n",
    "    return None,None\n",
    "\n",
    "rank, page = find_google_ranking(\"computational social science\",\"uva.nl\")\n",
    "\n",
    "if rank is None:\n",
    "    print(f\"Uva.nl was not listed in the first {how_many_pages_to_try} pages! :( We need to work on our SEO!\")\n",
    "else:\n",
    "    print(f\"UvA was number {rank} link for search result, on page {page} in the Google ranking!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced API: How toxic are YouTube comments? Combining YouTube API and Perspective API\n",
    "In this part of the guide, we will use YouTube API to collect comments from videos. \n",
    "\n",
    "### About authentication\n",
    "APIs often require users to sign up and use credentials. These are often based on \"API keys\" which link a call to the API to a particular user or registered application. There are many reason for APIs requiring authentifiatoin: by requiring credentials, API providers can control access to the data or services they offer, preventing unauthorized access and abuse, and to ensure rate limiting - that is, managing the load on the server by restricting the number of API calls from a single user or application within a given time frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sign up to the YouTube API at https://developers.google.com/youtube/v3 \n",
    "Read about the process on: https://developers.google.com/youtube/v3/getting-started\n",
    "\n",
    "Google offers a range of powerful and interesting APIs, both for data collection and analysis. Have a look and browse their offerings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching YouTube comments\n",
    "We will now use the YouTube API to fetch comments associated to a particular YouTube video.\n",
    "\n",
    "You'll find the API documentation here: https://developers.google.com/youtube/v3/docs/commentThreads \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 0...\n",
      "Getting page 1...\n",
      "Getting page 2...\n",
      "Getting page 3...\n",
      "Getting page 4...\n",
      "Done! Fetched 490 comments!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# The API key is your key to the YouTube API. You will neeed to get your own. To do so, visit https://developers.google.com/youtube/v3/getting-started \n",
    "api_key = #[YOUR API KEY]\n",
    "video_id = \"dQw4w9WgXcQ\"  \n",
    "# Replace with the ID of the video you are interested in. \n",
    "# You can find the ID by going to a video in Youtube, and getting the string after v= in the URL. For instance, i0EfLMe5FGk in https://www.youtube.com/watch?v=i0EfLMe5FGk\n",
    "\n",
    "url = f\"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "params = {\n",
    "    'part': 'snippet',\n",
    "    'videoId': video_id,\n",
    "    'maxResults': 100,  # max number of comments to fetch \n",
    "    'textFormat': 'plainText',\n",
    "    'key': api_key,\n",
    "}\n",
    "\n",
    "all_comments = []\n",
    "\n",
    "maximum_pages = 3 #How many pages to get at most\n",
    "\n",
    "for page in range(maximum_pages):\n",
    "    print(f\"Getting page {page}...\")\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        result_json = response.json()\n",
    "        all_comments.extend([item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in result_json.get('items', [])])\n",
    "\n",
    "        # Many APIs provide the result page by page. If there is another page, this API returns a nextPageToken, that we can\n",
    "        # send to the API to get the next page in line. If there are no more comments, there will be no such token.\n",
    "        if 'nextPageToken' in result_json:\n",
    "            params['pageToken'] = result_json['nextPageToken']\n",
    "            \n",
    "            # Ensure you don't hit the quota limits by adding a delay\n",
    "            time.sleep(1)\n",
    "        else: #No token, so no more pages\n",
    "            break\n",
    "    else:\n",
    "        print(\"Error: \", response.status_code)\n",
    "        break\n",
    "\n",
    "# Now 'all_comments' list contains all the comments from the video\n",
    "print(f\"Done. Fetched {len(all_comments)} comments!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 BILLION views for Never Gonna Give You Up!\\xa0 Amazing, crazy, wonderful! Rick ‚ô•Ô∏è', \"Greetings from Japan. This melody gives me a strong sense of d√©j√† vu. It feels like something is in the right place. It's a very good song. I definitely love this song and find it nostalgic.\", '13 years still watching this masterpiece \\U0001fae1\\U0001fae1', 'Ch√≠nh n√≥:)))', 'I love this song\\n\\n\\n\\n\\n\\nEdit:CAN I GET 178 LIKES?üó£Ô∏èüî•üî•']\n"
     ]
    }
   ],
   "source": [
    "#Print the first five comments\n",
    "print(all_comments[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Toxicity API\n",
    "\n",
    "The Perspective API, developed by Jigsaw and Google's Counter Abuse Technology team, is a tool that leverages machine learning to score toxicity in online conversation. The API provides various models to assess different aspects of conversations, like toxicity, severe toxicity, and threat, allowing developers and service providers to automatically moderate content that is harmful, abusive, or likely to drive users away, thus fostering healthier and more respectful online interactions.\n",
    "\n",
    "Perspective API is an example of an API that can be used to analyze your own data, rather than just fetching existing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perspective API is easiest to use through the Python package offered by Google. Many APIs offer Python packages to make it easier to use the API. APIs offer packages to simplify and streamline the interaction between the end-user's code and the API‚Äôs endpoints, abstracting the intricacies of HTTP requests, response handling, and error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the package\n",
    "!pip install googleapiclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The message scores 0.85850734 in toxicity\n"
     ]
    }
   ],
   "source": [
    "PERSPECTIVE_API_KEY = # YOUR API KEY HERE\n",
    "\n",
    "# The text string you want to analyze\n",
    "message = \"I fart in your general direction! Your mother was a hamster and your father smelt of elderberries!\"\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=PERSPECTIVE_API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': message },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "# Don't overload the API\n",
    "time.sleep(0.5)\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "toxicity = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "\n",
    "print(f\"The message scores {toxicity} in toxicity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-project: How toxic are the YouTube comments?\n",
    "Your task is to write a script that: \n",
    "\n",
    "1. Takes a list of YouTube video IDs and collects the first 100 comments from each video.\n",
    "2. Calculate the toxicity of each comment on the videos using Perspective API, and stores the result in a pandas Dataframe.\n",
    "3. Shows how toxic the comments are on average according to the Perspective API. (Use for instance np.mean() to calculate the average toxicity.)\n",
    "\n",
    "Select a couple of Youtube videos of your own choice, and use your code to analyze which of them has the most toxic comments. Reflect about the meaning of the findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from googleapiclient import discovery\n",
    "import pandas as pd\n",
    "\n",
    "youtube_api_key = [YOUR KEY HERE]\n",
    "PERSPECTIVE_API_KEY = [YOUR KEY HERE]\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=PERSPECTIVE_API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "#This function returns a list of comments (strings) associated to a video on Youtube\n",
    "def fetch_comments_for_video(video_id, max_comments_to_fetch=100):\n",
    "\n",
    "    print(f\"Fetching comments for video {video_id}.\")\n",
    "    \n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    params = {\n",
    "        'part': 'snippet',\n",
    "        'videoId': video_id,\n",
    "        'maxResults': max_comments_to_fetch,  \n",
    "        'textFormat': 'plainText',\n",
    "        'key': youtube_api_key,\n",
    "    }\n",
    "\n",
    "    all_comments = []\n",
    "    page = 0\n",
    "    while(True):\n",
    "        page+=1\n",
    "        \n",
    "        if len(all_comments)>=max_comments_to_fetch:\n",
    "            break\n",
    "        \n",
    "        print(f\"Getting page {page}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            result_json = response.json()\n",
    "            all_comments.extend([item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in result_json.get('items', [])])\n",
    "\n",
    "            # Many APIs provide the result page by page. If there is another page, this API returns a nextPageToken, that we can\n",
    "            # send to the API to get the next page in line. If there are no more comments, there will be no such token.\n",
    "            if 'nextPageToken' in result_json:\n",
    "                params['pageToken'] = result_json['nextPageToken']\n",
    "\n",
    "                # Ensure you don't hit the quota limits by adding a delay\n",
    "                time.sleep(1)\n",
    "            else: #No token mean no more pages, so we're done\n",
    "                break\n",
    "        else:\n",
    "            print(\"Error: \", response.status_code)\n",
    "            break\n",
    "\n",
    "    # Now 'all_comments' list contains all the comments from the video\n",
    "    print(f\"Done. Fetched {len(all_comments)} comments!\")\n",
    "    return all_comments\n",
    "\n",
    "#This function uses fetch_comments_for_video() to collect comments for several videos. \n",
    "# It takes a list of video ids and returns a dataframe  with the structure:\n",
    "# video_id | comment\n",
    "# \n",
    "def fetch_comments_for_videos(list_of_video_ids):\n",
    "    l = []\n",
    "    for video_id in list_of_video_ids:\n",
    "        comments = fetch_comments_for_video(video_id,)\n",
    "        for comment in comments:\n",
    "            l.append({'video_id':video_id,'comment': comment})\n",
    "                 \n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "# This measures the toxicity of a single message using the Perspective API\n",
    "def measure_toxicity_of_message(message):\n",
    "    \n",
    "    analyze_request = {\n",
    "      'comment': { 'text': message },\n",
    "      'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "    toxicity = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "\n",
    "    return toxicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments for video ATFwMO9CebA.\n",
      "Getting page 1...\n",
      "Getting page 2...\n",
      "Done. Fetched 199 comments!\n",
      "Fetching comments for video Wl6b5KnpmB4.\n",
      "Getting page 1...\n",
      "Done. Fetched 100 comments!\n"
     ]
    }
   ],
   "source": [
    "#Trump's state of the union vs Biden's state of the union\n",
    "comments = fetch_comments_for_videos(['ATFwMO9CebA','Wl6b5KnpmB4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the dataframe\n",
    "comments['toxicity'] = None\n",
    "comments['analyzed'] = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 comments left out of 299...\n",
      "280 comments left out of 299...\n",
      "270 comments left out of 299...\n",
      "260 comments left out of 299...\n",
      "250 comments left out of 299...\n",
      "240 comments left out of 299...\n",
      "230 comments left out of 299...\n",
      "220 comments left out of 299...\n",
      "210 comments left out of 299...\n",
      "200 comments left out of 299...\n",
      "190 comments left out of 299...\n",
      "180 comments left out of 299...\n",
      "170 comments left out of 299...\n",
      "160 comments left out of 299...\n",
      "150 comments left out of 299...\n",
      "140 comments left out of 299...\n",
      "130 comments left out of 299...\n",
      "120 comments left out of 299...\n",
      "110 comments left out of 299...\n",
      "100 comments left out of 299...\n",
      "90 comments left out of 299...\n",
      "80 comments left out of 299...\n",
      "70 comments left out of 299...\n",
      "60 comments left out of 299...\n",
      "50 comments left out of 299...\n",
      "40 comments left out of 299...\n",
      "30 comments left out of 299...\n",
      "20 comments left out of 299...\n",
      "10 comments left out of 299...\n",
      "We're done! Analysis failed for 18 of 299.\n"
     ]
    }
   ],
   "source": [
    "#This is a simple way of structuring your code when scraping many pages.\n",
    "i = 0\n",
    "nrfailed = 0\n",
    "while(True):    \n",
    "    #Fetch a random row\n",
    "    left_to_process = comments.loc[comments['analyzed']==False]\n",
    "    \n",
    "    if len(left_to_process)==0:\n",
    "        print(f\"We're done! Analysis failed for {nrfailed} of {len(comments)}.\")\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        comment = left_to_process.sample(1)\n",
    "        index = comment.index[0]\n",
    "        message = comment.comment.values[0]\n",
    "\n",
    "        #Keep track of progress. Every 10 measures, we print out a progress report\n",
    "        i+=1\n",
    "        if i%10==0:\n",
    "            print(f\"{len(comments.loc[comments['analyzed']==False])} comments left out of {len(comments)}...\")\n",
    "\n",
    "        try:\n",
    "            #Analyze toxicity\n",
    "            toxicity = measure_toxicity_of_message(message)\n",
    "            comments.loc[index,'toxicity'] = toxicity\n",
    "\n",
    "        except Exception as e:\n",
    "            #The API will fail for mant comments, for instance if they are too short or in the wrong language.\n",
    "            nrfailed+=1\n",
    "        finally:\n",
    "            comments.loc[index,'analyzed'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>analyzed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATFwMO9CebA</td>\n",
       "      <td>We love you president Trump!</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATFwMO9CebA</td>\n",
       "      <td>Take a lesson people! That's what a real presi...</td>\n",
       "      <td>0.045131</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATFwMO9CebA</td>\n",
       "      <td>üïäÔ∏èüè°üïäÔ∏èüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùü¶ÖüíùüíùüíùüíùüíùüíùüåßÔ∏èüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùü¶Öü¶Öü¶Öü¶Ö...</td>\n",
       "      <td>0.042657</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATFwMO9CebA</td>\n",
       "      <td>This is the real state of the union</td>\n",
       "      <td>0.028149</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATFwMO9CebA</td>\n",
       "      <td>How great. 2 people who ran for vice president...</td>\n",
       "      <td>0.05849</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Wl6b5KnpmB4</td>\n",
       "      <td>·ûÅ·üí·ûâ·ûª·üÜchea den ·ûÅ·üí·ûâ·ûª·üÜ·ûò·û∑·ûì·ûë·û∂·ûì·üã·ûî·û∂·ûì·ûõ·ûª·ûô·ûë·üÅ·ûü·ûº·ûò·ûõ·üÑ·ûÄ·ûá·ûº·ûî·üÉ·ûå·û∑...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Wl6b5KnpmB4</td>\n",
       "      <td>Refund your money if your airplanes delayed. W...</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Wl6b5KnpmB4</td>\n",
       "      <td>He slipped up 31:02 and was gonna say came tog...</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Wl6b5KnpmB4</td>\n",
       "      <td>Who‚Äôs the girl in the yellow dress she looks l...</td>\n",
       "      <td>0.157667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Wl6b5KnpmB4</td>\n",
       "      <td>What a pathetic scene</td>\n",
       "      <td>0.437201</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                                            comment  toxicity  \\\n",
       "0    ATFwMO9CebA                       We love you president Trump!  0.015205   \n",
       "1    ATFwMO9CebA  Take a lesson people! That's what a real presi...  0.045131   \n",
       "2    ATFwMO9CebA  üïäÔ∏èüè°üïäÔ∏èüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùü¶ÖüíùüíùüíùüíùüíùüíùüåßÔ∏èüíùüíùüíùüíùüíùüíùüíùüíùüíùüíùü¶Öü¶Öü¶Öü¶Ö...  0.042657   \n",
       "3    ATFwMO9CebA                This is the real state of the union  0.028149   \n",
       "4    ATFwMO9CebA  How great. 2 people who ran for vice president...   0.05849   \n",
       "..           ...                                                ...       ...   \n",
       "294  Wl6b5KnpmB4  ·ûÅ·üí·ûâ·ûª·üÜchea den ·ûÅ·üí·ûâ·ûª·üÜ·ûò·û∑·ûì·ûë·û∂·ûì·üã·ûî·û∂·ûì·ûõ·ûª·ûô·ûë·üÅ·ûü·ûº·ûò·ûõ·üÑ·ûÄ·ûá·ûº·ûî·üÉ·ûå·û∑...      None   \n",
       "295  Wl6b5KnpmB4  Refund your money if your airplanes delayed. W...  0.034277   \n",
       "296  Wl6b5KnpmB4  He slipped up 31:02 and was gonna say came tog...  0.071337   \n",
       "297  Wl6b5KnpmB4  Who‚Äôs the girl in the yellow dress she looks l...  0.157667   \n",
       "298  Wl6b5KnpmB4                              What a pathetic scene  0.437201   \n",
       "\n",
       "     analyzed  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4        True  \n",
       "..        ...  \n",
       "294      True  \n",
       "295      True  \n",
       "296      True  \n",
       "297      True  \n",
       "298      True  \n",
       "\n",
       "[299 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id\n",
       "ATFwMO9CebA    0.214887\n",
       "Wl6b5KnpmB4    0.202312\n",
       "Name: toxicity, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's compare the toxicities.\n",
    "# We look at the mean toxicity for the successfully analyzed comments:\n",
    "comments.loc[~comments['toxicity'].isna()].groupby(['video_id'])['toxicity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30daf35-ab8f-45af-9b73-d8216c0d7333",
   "metadata": {},
   "source": [
    "## Mini-project 2: Does the YouTube algorithm radicalize?\n",
    "\n",
    "We will now go through a more complex exercise, for a small research paper.\n",
    "\n",
    "Researchers have argued that the YouTube autoplay feature can lead to radicalization. The platform's recommendation system is designed to keep users engaged for as long as possible. The algorithm achieves this by suggesting content that it predicts the user will find interesting or compelling, based on their viewing history, search terms, and other interactions. \n",
    "\n",
    "However, critics argue that this approach can create a \"filter bubble,\" where users are only exposed to content and perspectives similar to those they have already encountered, thereby reinforcing existing beliefs and opinions. There are concerns that this can lead to the incremental presentation of more extreme content, as users are gradually exposed to increasingly radical viewpoints in a bid to sustain engagement. This phenomenon, sometimes referred to as \"algorithmic radicalization,\" has sparked debates about the ethical responsibilities of social media and content-sharing platforms and their role in the spread of misinformation, hate speech, and extremist ideologies. \n",
    "\n",
    "This would suggest that the comments on videos become more and more toxic as the algorithm proceeds!\n",
    "\n",
    "In this exercise, we are going to explore this hypothesis by tracing the YouTube autoplay feature from a given starting point.\n",
    "\n",
    "You will use the code we developed in our previous practical to fetch the comments on the video and evaluate their average toxicity. In case you did not complete that task, we will give you the solution here.\n",
    "\n",
    "While the Youtube API gives access to some features, the \"next video\" feature is not accessible through the API. For this, we therefore need to scrape the interface.\n",
    "\n",
    "#### Task: \n",
    "\n",
    "1. Choose a video to start from. This might for instance be a political video, where you would expect a radicalization loop to take place.\n",
    "\n",
    "2. Write code to repeatedly go to the \"next upcoming video\" and store the number of steps taken, the video id and the title. (Remember to pause between each fetch, so the page has time to load.)\n",
    "\n",
    "3. Store at least 10 steps of \"next video\", so that a trend can be spotted.\n",
    "\n",
    "4. Use your code from the previous practical, where you collected comments on YouTube videos using the API, and calculated the toxicity of each comment.\n",
    "\n",
    "5. For each video, calculate the average toxicity of the comments. \n",
    "\n",
    "6. Plot the trend: are the comments becoming more toxic? Do your findings fit with the YouTube autoplay radicalization hypothesis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa990f0c-c36f-4826-a17f-3a09b0cbfd4e",
   "metadata": {},
   "source": [
    "#### Code for analyzing how toxic text is\n",
    "We will use the Perspective API to measure toxicity. It's a machine learning API that classifies how incivil a social media message is. \n",
    "\n",
    "Make sure that you go through this code and understand it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "962ef7e9-d9d4-4078-87ba-c6420bba294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from googleapiclient import discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_key = [YOUR PERSPECTIVE API KEY HERE]\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=PERSPECTIVE_API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "#This function returns a list of comments (strings) associated to a video on Youtube\n",
    "def fetch_comments_for_video(video_id, max_comments_to_fetch=100):\n",
    "\n",
    "    print(f\"Fetching comments for video {video_id}.\")\n",
    "    \n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    params = {\n",
    "        'part': 'snippet',\n",
    "        'videoId': video_id,\n",
    "        'maxResults': 100,  \n",
    "        'textFormat': 'plainText',\n",
    "        'key': api_key,\n",
    "    }\n",
    "\n",
    "    all_comments = []\n",
    "    page = 0\n",
    "    while(True):\n",
    "        page+=1\n",
    "        \n",
    "        if len(all_comments)>=max_comments_to_fetch:\n",
    "            break\n",
    "        \n",
    "        print(f\"Getting page {page}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            result_json = response.json()\n",
    "            all_comments.extend([item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in result_json.get('items', [])])\n",
    "\n",
    "            # Many APIs provide the result page by page. If there is another page, this API returns a nextPageToken, that we can\n",
    "            # send to the API to get the next page in line. If there are no more comments, there will be no such token.\n",
    "            if 'nextPageToken' in result_json:\n",
    "                params['pageToken'] = result_json['nextPageToken']\n",
    "\n",
    "                # Ensure you don't hit the quota limits by adding a delay\n",
    "                time.sleep(1)\n",
    "            else: #No token mean no more pages, so we're done\n",
    "                break\n",
    "        else:\n",
    "            print(\"Error: \", response.status_code)\n",
    "            break\n",
    "\n",
    "    # Now 'all_comments' list contains all the comments from the video\n",
    "    print(f\"Done. Fetched {len(all_comments)} comments!\")\n",
    "    return all_comments\n",
    "\n",
    "# This measures the toxicity of a single message using the Perspective API\n",
    "def measure_toxicity_of_message(message):\n",
    "    \n",
    "    analyze_request = {\n",
    "      'comment': { 'text': message },\n",
    "      'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "    toxicity = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "\n",
    "    return toxicity\n",
    "\n",
    "\n",
    "# We first adapt our fetch_comments_for_videos() from last week so that it preserves our additional information about the video.\n",
    "# This function now takes a list of dicts (created by the function above), and fetches the comments for each video.\n",
    "# It produces a dataframe where each line is a comment, and the video information is included:\n",
    "# [{'step':0,'video_id': 'ATFwMO9CebA', 'President Trump 2018 State of the Union Address (C-SPAN)', 'comment':'Great speech!' }...]\n",
    "def fetch_comments_for_videos(list_of_videos,max_comments_to_fetch=100):\n",
    "    list_of_comments = []\n",
    "    for video in list_of_videos:\n",
    "        comments = fetch_comments_for_video(video['video_id'],max_comments_to_fetch)\n",
    "        for comment in comments:\n",
    "            list_of_comments.append(video | {'comment': comment}) #Add comment information to video information\n",
    "                 \n",
    "    return pd.DataFrame(list_of_comments)\n",
    "\n",
    "#This function takes a dataframe with comments, and analyzes each comment using perspective.\n",
    "# It returns an updated dataframe with toxicity information for each comment.\n",
    "def analyze_toxicity_of_comments(comments):\n",
    "    #Prepare the dataframe\n",
    "    comments['toxicity'] = None\n",
    "    comments['analyzed'] = False    \n",
    "    #This is a simple way of structuring your code when scraping many pages.\n",
    "    i = 0\n",
    "    nrfailed = 0\n",
    "    while(True):    \n",
    "        #Fetch a random row\n",
    "        left_to_process = comments.loc[comments['analyzed']==False]\n",
    "\n",
    "        if len(left_to_process)==0:\n",
    "            print(f\"We're done! Analysis failed for {nrfailed} of {len(comments)}.\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            comment = left_to_process.sample(1)\n",
    "            index = comment.index[0]\n",
    "            message = comment.comment.values[0]\n",
    "\n",
    "            #Keep track of progress. Every 10 measures, we print out a progress report\n",
    "            i+=1\n",
    "            if i%10==0:\n",
    "                print(f\"{len(comments.loc[comments['analyzed']==False])} comments left out of {len(comments)}...\")\n",
    "\n",
    "            try:\n",
    "                #Analyze toxicity\n",
    "                toxicity = measure_toxicity_of_message(message)\n",
    "                comments.loc[index,'toxicity'] = toxicity\n",
    "\n",
    "            except Exception as e:\n",
    "                #The API will fail for mant comments, for instance if they are too short or in the wrong language.\n",
    "                nrfailed+=1\n",
    "            finally:\n",
    "                comments.loc[index,'analyzed'] = True\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5371118f-8e31-4b3d-b8e2-464b1051d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous exercise, we used this to compare the state of the union speeches of Trump and Biden:\n",
    "\n",
    "#Fetch the comments: \n",
    "comments = fetch_comments_for_videos([{'president':'Trump','video_id':'ATFwMO9CebA'},{'president':'Biden','video_id':'Wl6b5KnpmB4'}],max_comments_to_fetch=50)\n",
    "\n",
    "#Analyze comments:\n",
    "comments = analyze_toxicity_of_comments(comments)\n",
    "\n",
    "#Calculate average toxicity:\n",
    "print(\"Average video comment toxicity:\")\n",
    "comments.loc[~comments['toxicity'].isna()].groupby(['president'])['toxicity'].mean()\n",
    "\n",
    "# Who was more toxic? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47c17c-c8fe-4937-9876-f4be8ad8e991",
   "metadata": {},
   "source": [
    "### Additional snippets of code to help you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a0bea33-4023-4d3d-ae0a-5cf83f2aa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the initial video URL. You can modify this to your preferred video\n",
    "driver.get('https://www.youtube.com/watch?v=dQw4w9WgXcQ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b940ef1-342a-446e-9230-862113f9b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cookie popup\n",
    "buttons = driver.find_elements(by=By.CSS_SELECTOR,value='button')\n",
    "\n",
    "for button in buttons:\n",
    "    if 'Reject' in button.text:\n",
    "        button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "571d3111-e595-4119-826c-07c25a965c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: PvHGl3L0WUI. Title: Karine Jean-Pierre has no answer for the crisis at the border.\n"
     ]
    }
   ],
   "source": [
    "#Get the title and id of the current video\n",
    "video_id = driver.current_url.split('=')[1]\n",
    "title = driver.find_element(by=By.CSS_SELECTOR,value='div#title h1')\n",
    "print(f\"Id: {video_id}. Title: {title.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3b7c8d6b-5194-478e-9177-dd803c671433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click next video\n",
    "nextvid = driver.find_element(by=By.CSS_SELECTOR,value='ytd-compact-video-renderer.ytd-watch-next-secondary-results-renderer a')\n",
    "nextvid.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00664fd8-cd9a-4435-91b8-a3653f02664b",
   "metadata": {},
   "source": [
    "### TASK: Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e1fff-1a65-488d-ad4d-70960c08fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a video_id to start with, and then takes nr_steps of \"next video\" from that video.\n",
    "# It returns a list of dicts, each containing with the step number, the video_id, and the title of the video\n",
    "#  e.g., [{'step':0,'video_id': 'ATFwMO9CebA', 'President Trump 2018 State of the Union Address (C-SPAN)' }...]\n",
    "\n",
    "def follow_next_video(start_video_id,nr_steps):\n",
    "#[YOUR CODE HERE]\n",
    "\n",
    "# This should result in the next 10 steps from the Biden state of the union video\n",
    "list_of_videos = follow_next_video('Wl6b5KnpmB4',10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ae1ab2b-115e-4b29-93df-05b8e15b0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_videos\n",
    "#Should look more or less like this:\n",
    "# [{'step': 0,\n",
    "#   'video_id': 'Wl6b5KnpmB4',\n",
    "#   'title': 'President Joe Biden delivers 2023 State of the Union address to Congress ‚Äî 2/7/23'},\n",
    "#  {'step': 1,\n",
    "#   'video_id': 'FtzvOZNyXdw',\n",
    "#   'title': \"Rise, fall of Sam Bankman-Fried, FTX at center of Michael Lewis' new book | 60 Minutes\"},\n",
    "#  {'step': 2,\n",
    "#   'video_id': 'XqwGt69pDXQ',\n",
    "#   'title': 'The Collapse Of FTX: Insiders Tell All | CNBC Documentary'},\n",
    "#  {'step': 3,\n",
    "#   'video_id': 'gqDCrdZVZnk',\n",
    "#   'title': 'The world‚Äôs most dangerous arms dealer | DW Documentary'},\n",
    "#  {'step': 4, 'video_id': 'G1p6rlDCxq0', 'title': 'World War One (ALL PARTS)'},\n",
    "# ...\n",
    "\n",
    "## What can you observe about the videos? \n",
    "# For instance, does the recommendation algorithm gets stuck in a cycle between two or three videos? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96a5bf8f-6542-4fe6-9cce-e8047e6c5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we're going to use our old code to analyze the data!\n",
    "\n",
    "#Fetch comments for the videos\n",
    "comments = fetch_comments_for_videos(list_of_videos,max_comments_to_fetch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a036ecf9-7f8d-461c-84a5-0ee79cc96a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate toxicity of each comment. This will take a while!\n",
    "comments = analyze_toxicity_of_comments(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6e570d8-eb00-4fed-b110-f0694a189933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the average toxicity over time. Is there a clear trend?\n",
    "comments.loc[~comments['toxicity'].isna()].groupby(['step'])['toxicity'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68490c0-d898-49ec-a8a3-dd8cb6fa3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# This function takes a video_id to start with, and then takes nr_steps of \"next video\" from that video.\n",
    "# It returns a list of dicts, each containing with the step number, the video_id, and the title of the video\n",
    "#  e.g., [{'step':0,'video_id': 'ATFwMO9CebA', 'President Trump 2018 State of the Union Address (C-SPAN)' }...]\n",
    "def follow_next_video(start_video_id,nr_steps):\n",
    "    l = []\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(f'https://www.youtube.com/watch?v={start_video_id}')  \n",
    "    time.sleep(8)\n",
    "\n",
    "    #Close popup\n",
    "    buttons = driver.find_elements(by=By.CSS_SELECTOR,value='button')\n",
    "    for button in buttons:\n",
    "        if 'Reject' in button.text:\n",
    "            button.click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Get information for first video\n",
    "    video_id = driver.current_url.split('=')[1]\n",
    "    title = driver.find_element(by=By.CSS_SELECTOR,value='div#title h1')    \n",
    "    l.append({'step':0, 'video_id': video_id, 'title': title.text})\n",
    "\n",
    "    # Click next video repeatedly\n",
    "    for step in range(nr_steps):\n",
    "        # Click next video\n",
    "        nextvid = driver.find_element(by=By.CSS_SELECTOR,value='ytd-compact-video-renderer.ytd-watch-next-secondary-results-renderer a')\n",
    "        nextvid.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        #Fetch video_id and video title\n",
    "        video_id = driver.current_url.split('=')[1]\n",
    "        title = driver.find_element(by=By.CSS_SELECTOR,value='div#title h1')    \n",
    "        l.append({'step':step+1, 'video_id': video_id, 'title': title.text})\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return l\n",
    "\n",
    "list_of_videos = follow_next_video('Wl6b5KnpmB4',10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
